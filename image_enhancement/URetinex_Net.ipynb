{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nFaoDDYSozM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "network folders:"
      ],
      "metadata": {
        "id": "HznxEppBaLrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# architecture.py\n",
        "import torch.nn as nn\n",
        "def get_batchnorm_layer(opts):\n",
        "    if opts.norm_layer == \"batch\":\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "    elif opts.layer == \"spectral_instance\":\n",
        "        norm_layer = nn.InstanceNorm2d\n",
        "    else:\n",
        "        print(\"not implemented\")\n",
        "        exit()\n",
        "    return norm_layer\n",
        "\n",
        "def get_conv2d_layer(in_c, out_c, k, s, p=0, dilation=1, groups=1):\n",
        "    return nn.Conv2d(in_channels=in_c,\n",
        "                    out_channels=out_c,\n",
        "                    kernel_size=k,\n",
        "                    stride=s,\n",
        "                    padding=p,dilation=dilation, groups=groups)\n",
        "\n",
        "def get_deconv2d_layer(in_c, out_c, k=1, s=1, p=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Upsample(scale_factor=2, mode=\"bilinear\"),\n",
        "        nn.Conv2d(\n",
        "            in_channels=in_c,\n",
        "            out_channels=out_c,\n",
        "            kernel_size=k,\n",
        "            stride=s,\n",
        "            padding=p\n",
        "        )\n",
        "    )\n",
        "\n",
        "class Identity(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "metadata": {
        "id": "SdDDedEjaVeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# illumination_adjustment\n",
        "import torch.nn as nn\n",
        "class Adjust_naive(nn.Module):\n",
        "    def __init__(self, opt):\n",
        "        super().__init__()\n",
        "        self.conv1 = get_conv2d_layer(in_c=2, out_c=32, k=5, s=1, p=2)\n",
        "        self.conv2 = get_conv2d_layer(in_c=32, out_c=32, k=5, s=1, p=2)\n",
        "        self.conv3 = get_conv2d_layer(in_c=32, out_c=32, k=5, s=1, p=2)\n",
        "        self.conv4 = get_conv2d_layer(in_c=32, out_c=1, k=5, s=1, p=2)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, l, alpha):\n",
        "        input = torch.cat([l, alpha], dim=1)\n",
        "        x = self.conv1(input)            \n",
        "        x = self.conv2(self.leaky_relu(x))   \n",
        "        x = self.conv3(self.leaky_relu(x))   \n",
        "        x = self.conv4(self.leaky_relu(x))\n",
        "        x = self.relu(x) \n",
        "        return x"
      ],
      "metadata": {
        "id": "ti82xQdNaffV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#illumination_enhance\n",
        "class Illumination_Alone(nn.Module):\n",
        "    def __init__(self, opts):\n",
        "        super().__init__()\n",
        "        self.opts = opts\n",
        "        self.conv1 = get_conv2d_layer(in_c=1, out_c=32, k=5, s=1, p=2)\n",
        "        self.conv2 = get_conv2d_layer(in_c=32, out_c=32, k=5, s=1, p=2)\n",
        "        self.conv3 = get_conv2d_layer(in_c=32, out_c=32, k=5, s=1, p=2)\n",
        "        self.conv4 = get_conv2d_layer(in_c=32, out_c=32, k=5, s=1, p=2)\n",
        "        self.conv5 = get_conv2d_layer(in_c=32, out_c=1, k=1, s=1, p=0)\n",
        "\n",
        "        self.leaky_relu_1 = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.leaky_relu_2 = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.leaky_relu_3 = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.leaky_relu_4 = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        #self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, l):\n",
        "        x = l\n",
        "        x1 = self.leaky_relu_1(self.conv1(x))\n",
        "        x2 = self.leaky_relu_2(self.conv2(x1))\n",
        "        x3 = self.leaky_relu_3(self.conv3(x2))\n",
        "        x4 = self.leaky_relu_4(self.conv4(x3))\n",
        "        x5 = self.relu(self.conv5(x4))\n",
        "        return x5\n"
      ],
      "metadata": {
        "id": "ERLMbcH5aqYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#math_module\n",
        "\n",
        "class P(nn.Module):\n",
        "    \"\"\"\n",
        "        to solve min(P) = ||I-PQ||^2 + γ||P-R||^2\n",
        "        this is a least square problem\n",
        "        how to solve?\n",
        "        P* = (gamma*R + I*Q) / (Q*Q + gamma)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, I, Q, R, gamma):\n",
        "        return ((I * Q + gamma * R) / (gamma + Q * Q))\n",
        "        \n",
        "class Q(nn.Module):\n",
        "    \"\"\"\n",
        "        to solve min(Q) = ||I-PQ||^2 + λ||Q-L||^2\n",
        "        Q* = (lamda*L + I*P) / (P*P + lamda)\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, I, P, L, lamda):\n",
        "        \n",
        "        IR = I[:, 0:1, :, :]\n",
        "        IG = I[:, 1:2, :, :]\n",
        "        IB = I[:, 2:3, :, :]\n",
        "\n",
        "        PR = P[:, 0:1, :, :]\n",
        "        PG = P[:, 1:2, :, :]\n",
        "        PB = P[:, 2:3, :, :]\n",
        "\n",
        "        return (IR*PR + IG*PG + IB*PB + lamda*L) / ((PR*PR + PG*PG + PB*PB) + lamda)"
      ],
      "metadata": {
        "id": "AHjpye7PXE6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restoration\n",
        "class HalfDnCNNSE(nn.Module):\n",
        "    def __init__(self, opts):\n",
        "        super().__init__()\n",
        "        self.opts = opts\n",
        "        \n",
        "        if self.opts.concat_L:\n",
        "            self.conv1 = get_conv2d_layer(in_c=3, out_c=32, k=3, s=1, p=1)\n",
        "            self.relu1 = nn.ReLU(inplace=True)\n",
        "            self.conv2 = get_conv2d_layer(in_c=1, out_c=32, k=3, s=1, p=1)\n",
        "            self.relu2 = nn.ReLU(inplace=True)\n",
        "        else:\n",
        "            self.conv1 = self.conv1 = get_conv2d_layer(in_c=3, out_c=64, k=3, s=1, p=1)\n",
        "            self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.se_layer = SELayer(channel=64)\n",
        "        self.conv3 = get_conv2d_layer(in_c=64, out_c=64, k=3, s=1, p=1)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "        self.conv4 = get_conv2d_layer(in_c=64, out_c=64, k=3, s=1, p=1)\n",
        "        self.relu4 = nn.ReLU(inplace=True)\n",
        "        self.conv5 = get_conv2d_layer(in_c=64, out_c=64, k=3, s=1, p=1)\n",
        "        self.relu5 = nn.ReLU(inplace=True)\n",
        "        self.conv6 = get_conv2d_layer(in_c=64, out_c=64, k=3, s=1, p=1)\n",
        "        self.relu6 = nn.ReLU(inplace=True)\n",
        "        self.conv7 = get_conv2d_layer(in_c=64, out_c=64, k=3, s=1, p=1)\n",
        "        self.relu7 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv8 = get_conv2d_layer(in_c=64, out_c=3, k=3, s=1, p=1)\n",
        "\n",
        "    def forward(self, r, l):\n",
        "        if self.opts.concat_L:\n",
        "            r_fs = self.relu1(self.conv1(r))\n",
        "            l_fs = self.relu2(self.conv2(l))\n",
        "            inf = torch.cat([r_fs, l_fs], dim=1)\n",
        "            se_inf = self.se_layer(inf)\n",
        "        else:\n",
        "            r_fs = self.relu1(self.conv1(r))\n",
        "            se_inf = self.se_layer(r_fs)\n",
        "        x1 = self.relu3(self.conv3(se_inf))\n",
        "        x2 = self.relu4(self.conv4(x1))\n",
        "        x3 = self.relu5(self.conv5(x2))\n",
        "        x4 = self.relu6(self.conv6(x3))\n",
        "        x5 = self.relu7(self.conv7(x4))\n",
        "        n = self.conv8(x5)\n",
        "        r_restore = r + n\n",
        "        return r_restore\n",
        "\n",
        "class SELayer(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SELayer, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ],
      "metadata": {
        "id": "tivLqgAPa5YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decom\n",
        "class Decom(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.decom = nn.Sequential(\n",
        "            get_conv2d_layer(in_c=3, out_c=32, k=3, s=1, p=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            get_conv2d_layer(in_c=32, out_c=32, k=3, s=1, p=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            get_conv2d_layer(in_c=32, out_c=32, k=3, s=1, p=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            get_conv2d_layer(in_c=32, out_c=4, k=3, s=1, p=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.decom(input)\n",
        "        R = output[:, 0:3, :, :]\n",
        "        L = output[:, 3:4, :, :]\n",
        "        return R, L"
      ],
      "metadata": {
        "id": "frlEVZMma8CW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "utils.py"
      ],
      "metadata": {
        "id": "nwQQ0x7LbIUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "def save_TensorImg(img_tensor, path, nrow=1):\n",
        "    torchvision.utils.save_image(img_tensor, path, nrow=nrow)\n",
        "\n",
        "\n",
        "def np_save_TensorImg(img_tensor, path):\n",
        "    img = np.squeeze(img_tensor.cpu().permute(0, 2, 3, 1).numpy())\n",
        "    im = Image.fromarray(np.clip(img * 255, 0, 255.0).astype('uint8'))\n",
        "    im.save(path, 'png')\n",
        "\n",
        "\n",
        "def define_modelR(opts):\n",
        "    if opts.R_model == \"HalfDnCNNSE\":\n",
        "        model_R = HalfDnCNNSE(opts)\n",
        "    return model_R\n",
        "\n",
        "\n",
        "def define_modelL(opts):\n",
        "    if opts.L_model == \"Illumination_Alone\":\n",
        "\n",
        "        model_L = Illumination_Alone(opts)\n",
        "    return model_L\n",
        "\n",
        "\n",
        "def define_modelA(opts):\n",
        "    if opts.A_model == \"naive\":\n",
        "\n",
        "        model_A = Adjust_naive(opts)\n",
        "    return model_A\n",
        "\n",
        "\n",
        "def load_initialize(model, decom_model_path):\n",
        "    if os.path.exists(decom_model_path):\n",
        "        checkpoint_Decom_low = torch.load(decom_model_path) # For CUDA, I don't have NVIDIA GPU.\n",
        "        # checkpoint_Decom_low = torch.load(decom_model_path,\n",
        "        #                                   map_location=torch.device('cpu'))\n",
        "        model.load_state_dict(checkpoint_Decom_low['state_dict']['model_R'])\n",
        "        # to freeze the params of Decomposition Model\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        return model\n",
        "    else:\n",
        "        print(\n",
        "            \"pretrained Initialize Model does not exist, check ---> %s \" % decom_model_path)\n",
        "        exit()\n",
        "\n",
        "\n",
        "def load_unfolding(unfolding_model_path):\n",
        "    if os.path.exists(unfolding_model_path):\n",
        "        checkpoint = torch.load(unfolding_model_path) # For CUDA, I don't have NVIDIA GPU.\n",
        "        # checkpoint = torch.load(unfolding_model_path,\n",
        "        #                                   map_location=torch.device('cpu'))\n",
        "        old_opts = checkpoint[\"opts\"]\n",
        "        model_R = define_modelR(old_opts)\n",
        "        model_L = define_modelL(old_opts)\n",
        "        model_R.load_state_dict(checkpoint['state_dict']['model_R'])\n",
        "        model_L.load_state_dict(checkpoint['state_dict']['model_L'])\n",
        "        for param_R in model_R.parameters():\n",
        "            param_R.requires_grad = False\n",
        "        for param_L in model_L.parameters():\n",
        "            param_L.requires_grad = False\n",
        "        return old_opts, model_R, model_L\n",
        "    else:\n",
        "        print(\n",
        "            \"pretrained Unfolding Model does not exist, check ---> %s\" % unfolding_model_path)\n",
        "        exit()\n",
        "\n",
        "\n",
        "def load_adjustment(adjust_model_path):\n",
        "    if os.path.exists(adjust_model_path):\n",
        "        # checkpoint_Adjust = torch.load(adjust_model_path)\n",
        "        checkpoint_Adjust = torch.load(adjust_model_path,\n",
        "                                       map_location=torch.device('cpu'))\n",
        "        model_A = define_modelA(checkpoint_Adjust['opts'])\n",
        "        model_A.load_state_dict(checkpoint_Adjust['state_dict']['model_A'])\n",
        "        print(\n",
        "            \" ===========>  loading pretrained Illumination Adjustment Model from: %s \" % adjust_model_path)\n",
        "        # to freeze the params of Decomposition Model\n",
        "        for param in model_A.parameters():\n",
        "            param.requires_grad = False\n",
        "        return model_A\n",
        "    else:\n",
        "        print(\n",
        "            \"pretrained Adjustment Model does not exist, check ---> %s\" % adjust_model_path)\n",
        "        exit()\n",
        "\n",
        "\n",
        "def param_all(model, net_input):\n",
        "    import torchsummary\n",
        "    shape = net_input.shape\n",
        "    torchsummary.summary(model, (shape[1], shape[2], shape[3]))\n",
        "\n",
        "\n",
        "def param_self_compute(model):\n",
        "    parmas = 0\n",
        "    for p in model.parameters():\n",
        "        # print(p)\n",
        "        parmas += p.numel()\n",
        "    return parmas\n"
      ],
      "metadata": {
        "id": "VoNpqhU4XFM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test.py"
      ],
      "metadata": {
        "id": "r6QzoY7wcI87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import time\n",
        "\n",
        "\n",
        "def one2three(x):\n",
        "    return torch.cat([x, x, x], dim=1).to(x)\n",
        "\n",
        "class Inference(nn.Module):\n",
        "    def __init__(self, opts):\n",
        "        super().__init__()\n",
        "        self.opts = opts\n",
        "        # loading decomposition model \n",
        "        self.model_Decom_low = Decom()\n",
        "        self.model_Decom_low = load_initialize(self.model_Decom_low, self.opts.Decom_model_low_path)\n",
        "        # loading R; old_model_opts; and L model\n",
        "        self.unfolding_opts, self.model_R, self.model_L= load_unfolding(self.opts.unfolding_model_path)\n",
        "        # loading adjustment model\n",
        "        self.adjust_model = load_adjustment(self.opts.adjust_model_path)\n",
        "        self.P = P()\n",
        "        self.Q = Q()\n",
        "        transform = [\n",
        "            transforms.ToTensor(),\n",
        "        ]\n",
        "        self.transform = transforms.Compose(transform)\n",
        "        print(self.model_Decom_low)\n",
        "        print(self.model_R)\n",
        "        print(self.model_L)\n",
        "        print(self.adjust_model)\n",
        "        #time.sleep(8)\n",
        "\n",
        "    def unfolding(self, input_low_img):\n",
        "        for t in range(self.unfolding_opts.round):      \n",
        "            if t == 0: # initialize R0, L0\n",
        "                P, Q = self.model_Decom_low(input_low_img)\n",
        "            else: # update P and Q\n",
        "                w_p = (self.unfolding_opts.gamma + self.unfolding_opts.Roffset * t)\n",
        "                w_q = (self.unfolding_opts.lamda + self.unfolding_opts.Loffset * t)\n",
        "                P = self.P(I=input_low_img, Q=Q, R=R, gamma=w_p)\n",
        "                Q = self.Q(I=input_low_img, P=P, L=L, lamda=w_q) \n",
        "            R = self.model_R(r=P, l=Q)\n",
        "            L = self.model_L(l=Q)\n",
        "        return R, L\n",
        "    \n",
        "    def lllumination_adjust(self, L, ratio):\n",
        "        ratio = torch.ones(L.shape).cuda() * self.opts.ratio\n",
        "        return self.adjust_model(l=L, alpha=ratio)\n",
        "    \n",
        "    def forward(self, input_low_img):\n",
        "        if torch.cuda.is_available():\n",
        "            input_low_img = input_low_img.cuda()\n",
        "        with torch.no_grad():\n",
        "            start = time.time()  \n",
        "            R, L = self.unfolding(input_low_img)\n",
        "            High_L = self.lllumination_adjust(L, self.opts.ratio)\n",
        "            I_enhance = High_L * R\n",
        "            p_time = (time.time() - start)\n",
        "        return I_enhance, p_time\n",
        "\n",
        "    def run(self, input_dir):\n",
        "          if not os.path.exists(input_dir):\n",
        "              print(f\"Directory '{input_dir}' does not exist!\")\n",
        "              return\n",
        "          if not os.path.isdir(input_dir):\n",
        "              print(f\"'{input_dir}' is not a directory!\")\n",
        "              return\n",
        "\n",
        "          if not os.path.exists(self.opts.output):\n",
        "              os.makedirs(self.opts.output)\n",
        "          run_starttime = time.time()\n",
        "          tmplst = os.listdir(input_dir)\n",
        "          for i, file_name in enumerate(tmplst):\n",
        "              if not file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                  continue\n",
        "\n",
        "              input_path = os.path.join(input_dir, file_name)\n",
        "              name = os.path.splitext(file_name)[0]\n",
        "              low_img = self.transform(Image.open(input_path)).unsqueeze(0)\n",
        "              enhance, p_time = self.forward(input_low_img=low_img)\n",
        "              # save_path = os.path.join(self.opts.output, file_name.replace(name,\n",
        "              #                                                              f\"{name}_{self.opts.ratio}_URetinexNet\"))\n",
        "              save_path = os.path.join(self.opts.output, name)\n",
        "              np_save_TensorImg(enhance, save_path)\n",
        "              if i % 50 == 0:\n",
        "                tmptime = time.time()-run_starttime\n",
        "                print(f\"{i}/{len(tmplst)}: {tmptime:.2f} seconds.\")\n",
        "\n",
        "          totaltime=time.time()-run_starttime\n",
        "          print(f\"Total time: {totaltime:.2f} seconds.\")\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "# if __name__ == \"__main__\":\n",
        "#     parser = argparse.ArgumentParser(description='Configure')\n",
        "#     # specify your data path here!\n",
        "#     parser.add_argument('--img_path', type=str, default=\"/content/drive/MyDrive/Mobile Robotics Project/Full Dataset/Converted/image_0\")\n",
        "#     parser.add_argument('--output', type=str, default=\"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/Datasets/Full Dataset/URetinex-Net/image_0\")\n",
        "#     # ratio are recommended to be 3-5, bigger ratio will lead to over-exposure \n",
        "#     parser.add_argument('--ratio', type=int, default=5)\n",
        "#     # model path\n",
        "#     parser.add_argument('--Decom_model_low_path', type=str, default=\"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/ckpt/init_low.pth\")\n",
        "#     parser.add_argument('--unfolding_model_path', type=str, default=\"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/ckpt/unfolding.pth\")\n",
        "#     parser.add_argument('--adjust_model_path', type=str, default=\"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/ckpt/L_adjust.pth\")\n",
        "#     parser.add_argument('--gpu_id', type=int, default=0)\n",
        "    \n",
        "#     opts = parser.parse_args()\n",
        "#     for k, v in vars(opts).items():\n",
        "#         print(k, v)\n",
        "    \n",
        "#     os.environ['CUDA_VISIBLE_DEVICES'] = str(opts.gpu_id)\n",
        "#     model = Inference(opts).cuda()\n",
        "#     model.run(opts.img_path)\n"
      ],
      "metadata": {
        "id": "4SjL7A0bXFXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse, os\n",
        "if __name__ == \"__main__\":\n",
        "    img_path = \"/content/drive/MyDrive/Mobile Robotics Project/Full Dataset/Converted/image_0\"\n",
        "    output = \"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/Datasets/Full Dataset/URetinex-Net/image_0\"\n",
        "    ratio = 5\n",
        "    decom_model_low_path = \"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/ckpt/init_low.pth\"\n",
        "    unfolding_model_path = \"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/ckpt/unfolding.pth\"\n",
        "    adjust_model_path = \"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/ckpt/L_adjust.pth\"\n",
        "    gpu_id = 0\n",
        "    \n",
        "    opts = argparse.Namespace(img_path=img_path, output=output, ratio=ratio,\n",
        "                              Decom_model_low_path=decom_model_low_path,\n",
        "                              unfolding_model_path=unfolding_model_path,\n",
        "                              adjust_model_path=adjust_model_path,\n",
        "                              gpu_id=gpu_id)\n",
        "    \n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = str(opts.gpu_id)\n",
        "    model = Inference(opts).cuda()\n",
        "    model.run(opts.img_path)\n"
      ],
      "metadata": {
        "id": "EPJ-CzYJgBu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    img_path = \"/content/drive/MyDrive/Mobile Robotics Project/Full Dataset/Converted/image_1\"\n",
        "    output = \"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/Datasets/Full Dataset/URetinex-Net/image_1\"\n",
        "    ratio = 5\n",
        "    decom_model_low_path = \"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/ckpt/init_low.pth\"\n",
        "    unfolding_model_path = \"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/ckpt/unfolding.pth\"\n",
        "    adjust_model_path = \"/content/drive/MyDrive/Mobile Robotics Project/Image Enhancement/ckpt/L_adjust.pth\"\n",
        "    gpu_id = 0\n",
        "    \n",
        "    opts = argparse.Namespace(img_path=img_path, output=output, ratio=ratio,\n",
        "                              Decom_model_low_path=decom_model_low_path,\n",
        "                              unfolding_model_path=unfolding_model_path,\n",
        "                              adjust_model_path=adjust_model_path,\n",
        "                              gpu_id=gpu_id)\n",
        "    \n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = str(opts.gpu_id)\n",
        "    model = Inference(opts).cuda()\n",
        "    model.run(opts.img_path)\n"
      ],
      "metadata": {
        "id": "HMwQ31jxhzjS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}